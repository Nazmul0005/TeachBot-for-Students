# ğŸ¤– TeachBot with Memory - RAG-based Educational Assistant

<div align="center">

![Python](https://img.shields.io/badge/python-v3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115.6-green.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.3.26-orange.svg)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-purple.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Docker](https://img.shields.io/badge/docker-ready-blue.svg)
![Deployed](https://img.shields.io/badge/deployed-live-brightgreen.svg)

*A sophisticated Teaching Assistant Chatbot with Long-Short Term Memory capabilities, built using FastAPI, LangChain, and vector databases. The system provides intelligent responses based on educational content while maintaining conversation context.*

</div>

---

## ğŸŒ **LIVE DEPLOYMENT - TEST THE SYSTEM NOW!**

<div align="center">

### ğŸš€ **Production Deployment on Render**

**ğŸ”— Live Application:** [https://teachbot-for-students-1.onrender.com/](https://teachbot-for-students-1.onrender.com/)

**ğŸ“– Interactive API Documentation:** [https://teachbot-for-students-1.onrender.com/docs](https://teachbot-for-students-1.onrender.com/docs)

**ğŸ¥ System Health Check:** [https://teachbot-for-students-1.onrender.com/health](https://teachbot-for-students-1.onrender.com/health)

---

### ğŸ§ª **Quick Test Commands**

**Test Bengali Query:**
```bash
curl -X POST "https://teachbot-for-students-1.onrender.com/simple-chat" \
     -H "Content-Type: application/json" \
     -d '{"message": "à¦®à¦à§à¦œà¦°à§€ à¦•à¦¿?"}'
```

**Test English Query:**
```bash
curl -X POST "https://teachbot-for-students-1.onrender.com/simple-chat" \
     -H "Content-Type: application/json" \
     -d '{"message": "What is the main theme of Oporichita story?"}'
```

**Test Memory-enabled Chat:**
```bash
curl -X POST "https://teachbot-for-students-1.onrender.com/chat" \
     -H "Content-Type: application/json" \
     -d '{
       "message": "à¦°à¦¬à§€à¦¨à§à¦¦à§à¦°à¦¨à¦¾à¦¥ à¦ à¦¾à¦•à§à¦° à¦•à§‡ à¦›à¦¿à¦²à§‡à¦¨?",
       "session_id": "test-session-123",
       "use_memory": true,
       "show_sources": true
     }'
```

</div>

---

## ğŸ¯ **For Judges & Evaluators**

<table>
<tr>
<td width="50%">

### ğŸ“‹ **Available Endpoints**
- **`POST /chat`** - Memory-enabled conversation
- **`POST /simple-chat`** - Quick responses
- **`GET /health`** - System status
- **`POST /evaluate`** - RAG evaluation
- **`GET /memory/{session_id}`** - Session retrieval
- **`DELETE /memory/{session_id}`** - Memory cleanup

</td>
<td width="50%">

### ğŸ” **Key Features to Test**
- **ğŸ‡§ğŸ‡© Bengali Language Support** - Native text processing
- **ğŸ§  Memory Persistence** - Conversation continuity
- **ğŸ“š Educational Content** - HSC Bangla literature
- **âš¡ Real-time Responses** - Fast inference
- **ğŸ“Š Source Attribution** - Transparent citations
- **ğŸ”„ Multi-model Support** - OpenAI & Groq integration

</td>
</tr>
</table>

### ğŸ“ **Sample Educational Queries to Test**

<details>
<summary><strong>ğŸ‡§ğŸ‡© Bengali Literature Questions</strong></summary>

```json
// Test these queries in the live system:
{
  "message": "à¦…à¦ªà¦°à¦¿à¦šà¦¿à¦¤à¦¾ à¦—à¦²à§à¦ªà§‡à¦° à¦®à§‚à¦² à¦¬à¦¿à¦·à¦¯à¦¼à¦¬à¦¸à§à¦¤à§ à¦•à§€?"
}

{
  "message": "à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦šà¦°à¦¿à¦¤à§à¦°à§‡à¦° à¦¬à¦¿à¦¶à§‡à¦·à¦¤à§à¦¬ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§à¦¨"
}

{
  "message": "à¦°à¦¬à§€à¦¨à§à¦¦à§à¦°à¦¨à¦¾à¦¥à§‡à¦° à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à§‡ à¦¨à¦¾à¦°à§€ à¦šà¦°à¦¿à¦¤à§à¦°à§‡à¦° à¦­à§‚à¦®à¦¿à¦•à¦¾"
}
```

</details>

<details>
<summary><strong>ğŸ‡ºğŸ‡¸ English Literature Questions</strong></summary>

```json
// Test these queries in the live system:
{
  "message": "What is the significance of dowry system in Tagore's stories?"
}

{
  "message": "Analyze the character development in Oporichita"
}

{
  "message": "How does Tagore portray women empowerment?"
}
```

</details>

---

<div align="center">

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-api-documentation) â€¢ [ğŸ”§ Installation](#ï¸-setup-guide) â€¢ [ğŸ³ Docker](#-docker-deployment) â€¢ [ğŸ“Š Evaluation](#-evaluation-matrix)

</div>

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ§  **Intelligent Memory System**
- **Dual Memory Architecture**: Short-term conversation history + Long-term knowledge base
- **Session Management**: Persistent conversation tracking
- **Context Awareness**: Maintains conversation flow and relevance

### ğŸŒ **Multi-language Support**
- **Bengali & English**: Native support for both languages
- **Cross-lingual Understanding**: Seamless language switching
- **Cultural Context**: Respects linguistic nuances

</td>
<td width="50%">

### ğŸ¤– **Advanced AI Integration**
- **Multiple LLM Support**: OpenAI GPT-4 & Groq Llama3
- **Smart Model Selection**: Automatic fallback mechanisms
- **Real-time Processing**: Fast response generation

### ğŸ“š **Document Processing**
- **Multi-format Support**: PDF, DOCX, TXT files
- **Intelligent Chunking**: Bengali-aware text segmentation
- **Vector Search**: FAISS-powered semantic retrieval

</td>
</tr>
</table>

---

## ğŸ“‹ Table of Contents

<details>
<summary>Click to expand navigation</summary>

- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ› ï¸ Setup Guide](#ï¸-setup-guide)
- [ğŸ“š Technology Stack](#-technology-stack)
- [ğŸ” Sample Queries](#-sample-queries-and-outputs)
- [ğŸ“– API Documentation](#-api-documentation)
- [ğŸ“Š Evaluation Matrix](#-evaluation-matrix)
- [ğŸ”§ Technical Implementation](#-technical-implementation-details)
- [ğŸ³ Docker Deployment](#-docker-deployment)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ¯ RAG System Analysis](#-rag-system-analysis---detailed-technical-report)

</details>

---

## ğŸš€ Quick Start

### âš¡ Docker (Recommended)

```bash
# Clone and setup
git clone <repository-url>
cd TeachBot-for-Students

# Configure environment
cp .env.example .env  # Add your API keys

# Launch with Docker
docker-compose up --build
```

**ğŸŒ Access Points:**
- **API**: http://localhost:5008
- **Documentation**: http://localhost:5008/docs
- **Health Check**: http://localhost:5008/health

### ğŸ§ª Test the System

```bash
curl -X POST "http://localhost:5008/simple-chat" \
     -H "Content-Type: application/json" \
     -d '{"message": "à¦®à¦à§à¦œà¦°à§€ à¦•à¦¿?"}'
```

---

## ğŸ› ï¸ Setup Guide

### ğŸ“‹ Prerequisites

<table>
<tr>
<td>

**ğŸ Runtime**
- Python 3.10+
- pip/conda

</td>
<td>

**ğŸ”‘ API Keys**
- OpenAI API Key
- Groq API Key (optional)
- HuggingFace Token

</td>
<td>

**ğŸ³ Deployment**
- Docker & Docker Compose
- 4GB+ RAM recommended

</td>
</tr>
</table>

### ğŸ”§ Local Installation

<details>
<summary>ğŸ“– Step-by-step installation guide</summary>

#### 1ï¸âƒ£ **Environment Setup**
```bash
# Clone repository
git clone <repository-url>
cd sdlfkaskjd

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

#### 2ï¸âƒ£ **Install Dependencies**
```bash
pip install -r requirements.txt
```

#### 3ï¸âƒ£ **Environment Configuration**
Create `.env` file:
```env
# Core API Keys
OPENAI_API_KEY=your_openai_api_key_here
GROQ_API_KEY=your_groq_api_key_here
HF_TOKEN=your_huggingface_token_here

# Google Cloud Document AI (Optional)
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account.json
PROJECT_ID=your_project_id
LOCATION=your_location
PROCESSOR_ID=your_processor_id
PROCESSOR_VERSION=your_processor_version
```

#### 4ï¸âƒ£ **Prepare Knowledge Base**
```bash
# Place documents in data/ directory
# Supported formats: PDF, DOCX, TXT

# Create vector embeddings
python create_memory_embedding.py
```

#### 5ï¸âƒ£ **Launch Application**
```bash
python main.py
# Available at: http://localhost:8000
```

</details>

---

## ğŸ“š Technology Stack

### ğŸ—ï¸ **Core Framework**
<table>
<tr>
<td width="25%"><strong>ğŸš€ FastAPI</strong><br><code>0.115.6</code></td>
<td width="25%"><strong>ğŸ¦„ Uvicorn</strong><br><code>0.34.0</code></td>
<td width="25%"><strong>ğŸ“Š Pydantic</strong><br><code>2.11.7</code></td>
<td width="25%"><strong>ğŸ”§ Python</strong><br><code>3.10+</code></td>
</tr>
</table>

### ğŸ¤– **AI/ML Libraries**
<table>
<tr>
<td width="33%">
<strong>ğŸ¦œ LangChain Ecosystem</strong><br>
â€¢ LangChain <code>0.3.26</code><br>
â€¢ LangChain Community <code>0.3.27</code><br>
â€¢ LangChain OpenAI <code>0.3.0</code><br>
â€¢ LangChain Groq <code>0.3.5</code>
</td>
<td width="33%">
<strong>ğŸ§  Vector & Embeddings</strong><br>
â€¢ FAISS-CPU <code>1.11.0</code><br>
â€¢ Sentence Transformers <code>5.0.0</code><br>
â€¢ OpenAI Embeddings<br>
â€¢ text-embedding-3-large
</td>
<td width="33%">
<strong>ğŸ“„ Document Processing</strong><br>
â€¢ PyPDF <code>5.7.0</code><br>
â€¢ PyMuPDF (Bengali support)<br>
â€¢ python-docx <code>1.1.2</code><br>
â€¢ python-pptx
</td>
</tr>
</table>

### ğŸ”® **LLM Providers**
<table>
<tr>
<td width="50%">
<strong>ğŸ§  OpenAI</strong><br>
â€¢ GPT-4 Integration<br>
â€¢ Advanced reasoning<br>
â€¢ Multilingual support
</td>
<td width="50%">
<strong>âš¡ Groq</strong><br>
â€¢ Fast inference API<br>
â€¢ Llama3 models<br>
â€¢ Cost-effective processing
</td>
</tr>
</table>

### ğŸ“Š **Evaluation & Analytics**
- **scikit-learn** `1.7.0` - ML metrics
- **NLTK** `3.9.1` - NLP processing
- **NumPy** `>=1.24.0` - Numerical computing
- **Pandas** - Data manipulation

### ğŸš€ **Deployment & Infrastructure**
- **ğŸ³ Docker** - Containerization
- **ğŸŒ Nginx** - Reverse proxy & load balancing
- **ğŸ¦„ Gunicorn** - WSGI HTTP Server

---

## ğŸ” Sample Queries and Outputs

### ğŸ‡§ğŸ‡© Bengali Queries

<details>
<summary><strong>ğŸ“ Query 1: "à¦®à¦à§à¦œà¦°à§€ à¦•à¦¿?"</strong></summary>

**Input:**
```json
{
  "message": "à¦®à¦à§à¦œà¦°à§€ à¦•à¦¿?",
  "session_id": "demo-session",
  "use_memory": true
}
```

**Output:**
```json
{
  "answer": "à¦®à¦à§à¦œà¦°à§€ à¦¹à¦²à§‹ à¦•à¦¿à¦¶à¦²à¦¯à¦¼à¦¯à§à¦•à§à¦¤ à¦•à¦šà¦¿ à¦¡à¦¾à¦² à¦¬à¦¾ à¦®à§à¦•à§à¦²à¥¤ à¦à¦Ÿà¦¿ à¦—à¦¾à¦›à§‡à¦° à¦¨à¦¤à§à¦¨ à¦ªà¦¾à¦¤à¦¾ à¦“ à¦•à§à¦à¦¡à¦¼à¦¿à¦¸à¦¹ à¦›à§‹à¦Ÿ à¦¡à¦¾à¦²à¦•à§‡ à¦¬à§‹à¦à¦¾à¦¯à¦¼à¥¤ à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à§‡ à¦à¦Ÿà¦¿ à¦ªà§à¦°à¦¾à¦¯à¦¼à¦‡ à¦¸à§Œà¦¨à§à¦¦à¦°à§à¦¯ à¦“ à¦¨à¦¬à§€à¦¨à¦¤à¦¾à¦° à¦ªà§à¦°à¦¤à§€à¦• à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤ à¦¹à¦¯à¦¼à¥¤",
  "sources": [
    {
      "source_id": 1,
      "page": 5,
      "content_preview": "à¦®à¦à§à¦œà¦°à§€ à¦¶à¦¬à§à¦¦à§‡à¦° à¦…à¦°à§à¦¥: à¦•à¦¿à¦¶à¦²à¦¯à¦¼à¦¯à§à¦•à§à¦¤ à¦•à¦šà¦¿ à¦¡à¦¾à¦²à¥¤ à¦®à§à¦•à§à¦²...",
      "metadata": {"source": "data/HSC26-Bangla1st-Paper.txt", "page": 5}
    }
  ],
  "model_used": "OpenAI GPT-4 (Auto)",
  "processing_time": 1.23,
  "session_id": "demo-session",
  "conversation_length": 1
}
```

</details>

<details>
<summary><strong>ğŸ“š Query 2: "à¦°à¦¬à§€à¦¨à§à¦¦à§à¦°à¦¨à¦¾à¦¥ à¦ à¦¾à¦•à§à¦° à¦•à§‡ à¦›à¦¿à¦²à§‡à¦¨?"</strong></summary>

**Input:**
```json
{
  "message": "à¦°à¦¬à§€à¦¨à§à¦¦à§à¦°à¦¨à¦¾à¦¥ à¦ à¦¾à¦•à§à¦° à¦•à§‡ à¦›à¦¿à¦²à§‡à¦¨?"
}
```

**Output:**
```json
{
  "answer": "à¦°à¦¬à§€à¦¨à§à¦¦à§à¦°à¦¨à¦¾à¦¥ à¦ à¦¾à¦•à§à¦° (à§§à§®à§¬à§§-à§§à§¯à§ªà§§) à¦›à¦¿à¦²à§‡à¦¨ à¦¬à¦¾à¦‚à¦²à¦¾ à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à§‡à¦° à¦…à¦¨à§à¦¯à¦¤à¦® à¦¶à§à¦°à§‡à¦·à§à¦  à¦•à¦¬à¦¿, à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à¦¿à¦•, à¦¦à¦¾à¦°à§à¦¶à¦¨à¦¿à¦•, à¦¶à¦¿à¦•à§à¦·à¦¾à¦¬à¦¿à¦¦ à¦“ à¦¸à¦‚à¦—à§€à¦¤à¦•à¦¾à¦°à¥¤ à¦¤à¦¿à¦¨à¦¿ à§§à§¯à§§à§© à¦¸à¦¾à¦²à§‡ 'à¦—à§€à¦¤à¦¾à¦à§à¦œà¦²à¦¿' à¦•à¦¾à¦¬à§à¦¯à¦—à§à¦°à¦¨à§à¦¥à§‡à¦° à¦œà¦¨à§à¦¯ à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à§‡ à¦¨à§‹à¦¬à§‡à¦² à¦ªà§à¦°à¦¸à§à¦•à¦¾à¦° à¦²à¦¾à¦­ à¦•à¦°à§‡à¦¨à¥¤ à¦¤à¦¿à¦¨à¦¿ à¦¬à¦¿à¦¶à§à¦¬à¦­à¦¾à¦°à¦¤à§€ à¦¬à¦¿à¦¶à§à¦¬à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼à§‡à¦° à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦¤à¦¾ à¦à¦¬à¦‚ à¦¬à¦¾à¦‚à¦²à¦¾ à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à§‡ à¦†à¦§à§à¦¨à¦¿à¦•à¦¤à¦¾à¦° à¦ªà¦¥à¦ªà§à¦°à¦¦à¦°à§à¦¶à¦•à¥¤",
  "model_used": "OpenAI GPT-4 (Auto)",
  "processing_time": 1.45,
  "session_id": "auto-generated-uuid",
  "conversation_length": 1
}
```

</details>

### ğŸ‡ºğŸ‡¸ English Queries

<details>
<summary><strong>ğŸ“– Query 3: "What is the main theme of 'Oporichita' story?"</strong></summary>

**Input:**
```json
{
  "message": "What is the main theme of 'Oporichita' story?",
  "k_docs": 5,
  "show_sources": true
}
```

**Output:**
```json
{
  "answer": "The main theme of 'Oporichita' (The Stranger) by Rabindranath Tagore revolves around the dowry system and its dehumanizing effects on society. The story critiques the practice of treating marriage as a commercial transaction and highlights issues of self-respect, dignity, and social reform. It portrays the conflict between traditional patriarchal values and emerging progressive ideals, particularly through the character of Kallyani who refuses to compromise her dignity for marriage.",
  "sources": [
    {
      "source_id": 1,
      "page": 18,
      "content_preview": "à¦…à¦ªà¦°à¦¿à¦šà¦¿à¦¤à¦¾ à¦—à¦²à§à¦ªà§‡ à¦…à¦ªà¦°à¦¿à¦šà¦¿à¦¤à¦¾ à¦¬à¦¿à¦¶à§‡à¦·à¦£à§‡à¦° à¦†à¦¡à¦¼à¦¾à¦²à§‡ à¦¯à§‡ à¦¬à¦²à¦¿à¦·à§à¦  à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦¤à§à¦¬à§‡à¦° à¦…à¦§à¦¿à¦•à¦¾à¦°à§€ à¦¨à¦¾à¦°à§€à¦° à¦•à¦¾à¦¹à¦¿à¦¨à¦¿ à¦¬à¦°à§à¦£à¦¿à¦¤ à¦¹à¦¯à¦¼à§‡à¦›ï¿½ï¿½...",
      "metadata": {"source": "data/HSC26-Bangla1st-Paper.txt", "page": 18}
    }
  ],
  "model_used": "OpenAI GPT-4 (Auto)",
  "processing_time": 1.67,
  "session_id": "auto-generated-uuid",
  "conversation_length": 1
}
```

</details>

### ğŸ”„ Follow-up Conversation

<details>
<summary><strong>ğŸ’¬ Contextual Query: "à¦à¦‡ à¦—à¦²à§à¦ªà§‡ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦šà¦°à¦¿à¦¤à§à¦°à§‡à¦° à¦¬à¦¿à¦¶à§‡à¦·à¦¤à§à¦¬ à¦•à§€?"</strong></summary>

**Input:**
```json
{
  "message": "à¦à¦‡ à¦—à¦²à§à¦ªà§‡ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦šà¦°à¦¿à¦¤à§à¦°à§‡à¦° à¦¬à¦¿à¦¶à§‡à¦·à¦¤à§à¦¬ à¦•à§€?",
  "session_id": "same-session-as-previous",
  "use_memory": true
}
```

**Output:**
```json
{
  "answer": "à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦šà¦°à¦¿à¦¤à§à¦°à§‡à¦° à¦ªà§à¦°à¦§à¦¾à¦¨ à¦¬à¦¿à¦¶à§‡à¦·à¦¤à§à¦¬ à¦¹à¦²à§‹ à¦¤à¦¾à¦° à¦†à¦¤à§à¦®à¦¸à¦®à§à¦®à¦¾à¦¨à¦¬à§‹à¦§ à¦“ à¦ªà§à¦°à¦¤à¦¿à¦¬à¦¾à¦¦à§€ à¦®à¦¾à¦¨à¦¸à¦¿à¦•à¦¤à¦¾à¥¤ à¦ªà§‚à¦°à§à¦¬à¦¬à¦°à§à¦¤à§€ à¦†à¦²à§‹à¦šà¦¨à¦¾à¦¯à¦¼ à¦‰à¦²à§à¦²à¦¿à¦–à¦¿à¦¤ 'à¦…à¦ªà¦°à¦¿à¦šà¦¿à¦¤à¦¾' à¦—à¦²à§à¦ªà§‡ à¦•à¦²à§à¦¯à¦¾à¦£à§€ à¦à¦•à¦œà¦¨ à¦¶à¦¿à¦•à§à¦·à¦¿à¦¤, à¦¸à§à¦¬à¦¾à¦§à§€à¦¨à¦šà§‡à¦¤à¦¾ à¦¨à¦¾à¦°à§€ à¦¯à¦¿à¦¨à¦¿ à¦¯à§Œà¦¤à§à¦•à§‡à¦° à¦…à¦ªà¦®à¦¾à¦¨à§‡à¦° à¦¬à¦¿à¦°à§à¦¦ï¿½ï¿½à¦§à§‡ à¦°à§à¦–à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¾à¦¨à¥¤ à¦¤à¦¿à¦¨à¦¿ à¦¬à¦¿à¦¯à¦¼à§‡ à¦¨à¦¾ à¦•à¦°à¦¾à¦° à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤ à¦¨à§‡à¦¨ à¦à¦¬à¦‚ à¦¦à§‡à¦¶à¦¸à§‡à¦¬à¦¾à¦¯à¦¼ à¦¨à¦¿à¦œà§‡à¦•à§‡ à¦¨à¦¿à¦¯à¦¼à§‹à¦œà¦¿à¦¤ à¦•à¦°à§‡à¦¨à¥¤ à¦¤à¦¾à¦° à¦šà¦°à¦¿à¦¤à§à¦°à§‡ à¦¨à¦¾à¦°à§€à¦° à¦•à§à¦·à¦®à¦¤à¦¾à¦¯à¦¼à¦¨ à¦“ à¦¸à¦¾à¦®à¦¾à¦œà¦¿à¦• à¦¸à¦‚à¦¸à§à¦•à¦¾à¦°à§‡à¦° à¦†à¦¦à¦°à§à¦¶ à¦ªà§à¦°à¦¤à¦¿à¦«à¦²à¦¿à¦¤ à¦¹à¦¯à¦¼à§‡à¦›à§‡à¥¤",
  "model_used": "OpenAI GPT-4 (Auto)",
  "processing_time": 1.34,
  "session_id": "same-session-as-previous",
  "conversation_length": 2
}
```

</details>

---

## ğŸ“– API Documentation

### ğŸŒ Base URLs
<table>
<tr>
<td><strong>ğŸ  Local Development</strong></td>
<td><code>http://localhost:8000</code></td>
</tr>
<tr>
<td><strong>ğŸ³ Docker Deployment</strong></td>
<td><code>http://localhost:5008</code></td>
</tr>
</table>

### ğŸ“š Interactive Documentation
- **ğŸ¨ Swagger UI**: `/docs` - Interactive API explorer
- **ğŸ“– ReDoc**: `/redoc` - Clean documentation interface

### ğŸ”— Main Endpoints

#### ğŸ’¬ **Chat with Memory**
```http
POST /chat
```

<details>
<summary>ğŸ“‹ Request/Response Details</summary>

**Request Body:**
```json
{
  "message": "Your question here",
  "session_id": "optional-session-id",
  "llm_model": "auto|openai|groq",
  "k_docs": 5,
  "show_sources": true,
  "use_memory": true
}
```

**Response:**
```json
{
  "answer": "Bot's intelligent response",
  "sources": [
    {
      "source_id": 1,
      "page": 5,
      "content_preview": "Relevant content snippet...",
      "metadata": {"source": "document.txt", "page": 5}
    }
  ],
  "model_used": "OpenAI GPT-4 (Auto)",
  "processing_time": 1.23,
  "session_id": "uuid-session-id",
  "conversation_length": 1
}
```

</details>

#### ğŸ’­ **Simple Chat**
```http
POST /simple-chat
```

<details>
<summary>ğŸ“‹ Request/Response Details</summary>

**Request Body:**
```json
{
  "message": "Your question here"
}
```

**Response:**
```json
{
  "answer": "Simple response without memory",
  "model_used": "OpenAI GPT-4",
  "processing_time": 0.89
}
```

</details>

#### ğŸ¥ **Health Check**
```http
GET /health
```

<details>
<summary>ğŸ“‹ Response Details</summary>

```json
{
  "status": "healthy",
  "vectorstore_loaded": true,
  "available_models": ["openai", "groq"],
  "active_sessions": 5,
  "timestamp": "2024-01-01T12:00:00Z",
  "system_info": {
    "memory_usage": "2.1GB",
    "cpu_usage": "15%",
    "uptime": "2h 30m"
  }
}
```

</details>

#### ğŸ§  **Memory Management**
```http
GET /memory/{session_id}     # Retrieve session memory
DELETE /memory/{session_id}  # Clear session memory
```

#### ğŸ“Š **Evaluation**
```http
POST /evaluate
```

---

## ğŸ“Š Evaluation Matrix

### ğŸ¯ Evaluation Metrics

Our comprehensive RAG evaluation system uses multiple metrics to ensure quality:

<table>
<tr>
<td width="50%">

#### ğŸ¯ **Groundedness Metrics**
- **ğŸ“Š Context Overlap Score**: Answer-context alignment
- **ğŸ“ Citation Score**: Source attribution quality
- **ğŸš« Hallucination Score**: Factual accuracy (lower = better)
- **ğŸ”— Semantic Similarity**: Meaning preservation

</td>
<td width="50%">

#### ğŸ” **Relevance Metrics**
- **ğŸ¯ Query-Document Similarity**: Retrieval accuracy
- **ğŸ“ˆ Document Ranking Score**: Result quality (DCG-like)
- **âœ… Retrieval Precision**: Relevant document ratio
- **ğŸ“‹ Coverage Score**: Topic comprehensiveness

</td>
</tr>
</table>

### ğŸ“ˆ Sample Evaluation Results

<details>
<summary>ğŸ“Š View detailed evaluation report</summary>

```json
{
  "timestamp": "2024-01-01T12:00:00Z",
  "evaluation_summary": {
    "total_questions": 25,
    "average_groundedness": 0.847,
    "average_relevance": 0.792,
    "average_response_time": 1.34,
    "success_rate": 96.0
  },
  "performance_breakdown": {
    "bengali_queries": {
      "count": 15,
      "avg_groundedness": 0.863,
      "avg_relevance": 0.801,
      "avg_response_time": 1.28
    },
    "english_queries": {
      "count": 10,
      "avg_groundedness": 0.824,
      "avg_relevance": 0.778,
      "avg_response_time": 1.42
    }
  },
  "individual_results": [
    {
      "question": "à¦®à¦à§à¦œà¦°ï¿½ï¿½ï¿½ à¦•à¦¿?",
      "groundedness_score": 0.89,
      "relevance_score": 0.85,
      "response_time": 1.23,
      "model_used": "OpenAI GPT-4 (Auto)",
      "num_sources": 3,
      "confidence": 0.92
    }
  ]
}
```

</details>

### ğŸ§ª Running Evaluations

```bash
# Run comprehensive evaluation
python others/rag_evaluator.py

# Run specific evaluation type
python others/rag_evaluator.py --type groundedness
python others/rag_evaluator.py --type relevance
```

---

## ğŸ”§ Technical Implementation Details

### ğŸ“„ Text Extraction Methods and Challenges

<details>
<summary><strong>ğŸ” What method or library did you use to extract text, and why?</strong></summary>

**ğŸ› ï¸ Multi-layered Extraction Approach:**

1. **ğŸ”§ PyMuPDF (fitz)** for PDF processing:
   - âœ… Superior Bengali/multilingual text extraction
   - âœ… Better handling of complex layouts and fonts
   - âœ… Preserves text structure and formatting

2. **ğŸ“„ python-docx** for DOCX files:
   - âœ… Extracts both paragraphs and table content
   - âœ… Handles complex document structures

3. **ğŸ“ Multiple encoding support** for TXT files:
   - âœ… UTF-8, UTF-8-sig, Latin-1, CP1252
   - âœ… Fallback mechanisms for encoding detection

**âš ï¸ Formatting Challenges Faced:**
- ğŸ”¤ Bengali text encoding issues with standard PDF libraries
- ğŸ“Š Complex table structures in DOCX files
- ğŸŒ Mixed language content requiring special handling
- âšª Whitespace preservation for proper text chunking

**ğŸ’¡ Solution Implemented:**
```python
# Enhanced PDF processing with PyMuPDF
text = page.get_text("text", flags=fitz.TEXT_PRESERVE_WHITESPACE)
```

</details>

### âœ‚ï¸ Chunking Strategy

<details>
<summary><strong>ğŸ§© What chunking strategy did you choose and why?</strong></summary>

**ğŸ”„ Recursive Character Text Splitter** with Bengali-aware optimization:

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # Larger chunks for better context
    chunk_overlap=200,  # More overlap for continuity
    separators=["\n\n", "\n", "à¥¤", ".", " ", ""]  # Bangla-aware separators
)
```

**âœ… Why this strategy works well:**
1. **ğŸ“ Larger chunk size (1500)**: Preserves more context for educational content
2. **ğŸ”— Significant overlap (200)**: Ensures continuity across chunks
3. **ğŸ‡§ğŸ‡© Bengali-aware separators**: Respects Bengali sentence structure (à¥¤)
4. **ğŸ—ï¸ Hierarchical splitting**: Maintains document structure integrity
5. **ğŸ§  Semantic preservation**: Keeps related concepts together

</details>

### ğŸ¤– Embedding Model Selection

<details>
<summary><strong>ğŸ¯ What embedding model did you use and why?</strong></summary>

**ğŸ¥‡ Primary Model: OpenAI's text-embedding-3-large**

```python
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-large",
    openai_api_key=OPENAI_API_KEY
)
```

**ğŸ¥ˆ Fallback Model: HuggingFace**
```python
embedding_model = HuggingFaceEmbeddings(
    model_name='sentence-transformers/all-MiniLM-L6-v2'
)
```

**ğŸŒŸ Why text-embedding-3-large:**
1. **ğŸŒ Multilingual support**: Excellent for Bengali and English
2. **ğŸ“Š Large dimension space**: 3072 dimensions for nuanced representations
3. **ğŸ“š Educational content optimization**: Trained on diverse academic content
4. **ğŸ§  Semantic understanding**: Captures contextual relationships effectively
5. **ğŸš€ Latest technology**: State-of-the-art performance

**ğŸ”¬ How it captures meaning:**
- ğŸ”„ Transforms text into high-dimensional vectors
- ğŸ¯ Similar concepts cluster together in vector space
- ğŸ”— Captures semantic relationships beyond keyword matching
- ğŸ“ Handles synonyms and contextual variations

</details>

### ğŸ” Similarity Comparison and Storage

<details>
<summary><strong>âš¡ How are you comparing queries with stored chunks?</strong></summary>

**ğŸ—ï¸ FAISS (Facebook AI Similarity Search)** with **ğŸ“ cosine similarity**:

```python
# Vector store creation
db = FAISS.from_documents(text_chunks, embedding_model)

# Retrieval with similarity search
retriever = vectorstore.as_retriever(search_kwargs={
    'k': k_docs,
    'fetch_k': k_docs * 2
})
```

**âœ… Why FAISS + Cosine Similarity:**
1. **âš¡ Efficiency**: Fast approximate nearest neighbor search
2. **ğŸ“ˆ Scalability**: Handles large document collections
3. **ğŸ’¾ Memory optimization**: Efficient storage and retrieval
4. **ğŸ“ Cosine similarity**: Measures semantic similarity regardless of vector magnitude
5. **ğŸ­ Production-ready**: Battle-tested in real-world applications

**ğŸ—ï¸ Storage Architecture:**
- **ğŸ—„ï¸ Vector Database**: FAISS for semantic search
- **ğŸ“‹ Metadata Storage**: Document source, page numbers, content previews
- **ğŸ”„ Hierarchical Fallback**: Multiple vector store paths for reliability

</details>

### ğŸ¯ Meaningful Comparison and Vague Query Handling

<details>
<summary><strong>ğŸ¤” How do you handle vague queries and ensure meaningful comparison?</strong></summary>

**ğŸ§  Memory-Aware Prompting:**
```python
# Enhanced question with conversation context
if use_memory and chat_history:
    enhanced_question = f"à¦•à¦¥à§‹à¦ªà¦•à¦¥à¦¨à§‡à¦° à¦‡à¦¤à¦¿à¦¹à¦¾à¦¸:\n{chat_history}\n\nà¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦ªà§à¦°à¦¶à§à¦¨: {request.message}"
```

**ğŸ”„ Multi-stage Retrieval:**
- ğŸ“Š Fetch more candidates (`fetch_k = k_docs * 2`)
- ğŸ“ˆ Re-rank based on relevance
- ğŸ¯ Filter by confidence threshold

**ğŸ­ Context-Aware Prompting:**
```python
MEMORY_AWARE_PROMPT_TEMPLATE = """
à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦¸à¦¹à¦¾à¦¯à¦¼à¦• à¦¶à¦¿à¦•à§à¦·à¦¾ à¦¸à¦¹à¦•à¦¾à¦°à§€ à¦¯à¦¿à¦¨à¦¿ à¦¶à¦¿à¦•à§à¦·à¦¾à¦°à§à¦¥à§€à¦¦à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦§à¦¾à¦°à¦¾à¦¬à¦¾à¦¹à¦¿à¦• à¦•à¦¥à§‹à¦ªà¦•à¦¥à¦¨à§‡ à¦…à¦‚à¦¶ à¦¨à§‡à¦¨à¥¤

à¦¨à¦¿à¦®à§à¦¨à¦²à¦¿à¦–à¦¿à¦¤ à¦¨à¦¿à¦¯à¦¼à¦®à¦—à§à¦²à¦¿ à¦…à¦¨à§à¦¸à¦°à¦£ à¦•à¦°à§à¦¨:
1. à¦•à¦¥à§‹à¦ªà¦•à¦¥à¦¨à§‡à¦° à¦ªà§à¦°à¦¸à¦™à§à¦— à¦¬à¦œà¦¾à¦¯à¦¼ à¦°à¦¾à¦–à§à¦¨
2. à¦ªà§‚ï¿½ï¿½à§à¦¬à¦¬à¦°à§à¦¤à§€ à¦ªà§à¦°à¦¶à§à¦¨ à¦“ à¦‰à¦¤à§à¦¤à¦°à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦¾à¦®à¦à§à¦œà¦¸à§à¦¯ à¦°à¦¾à¦–à§à¦¨
3. à¦ªà§à¦°à¦¦à¦¤à§à¦¤ à¦ªà§à¦°à¦¸à¦™à§à¦— à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§à¦¨
4. à¦ªà§à¦°à¦¶à§à¦¨ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¹à¦²à§‡ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¨
5. à¦ªà§à¦°à¦¶à§à¦¨ à¦‡à¦‚à¦°à§‡à¦œà¦¿à¦¤à§‡ à¦¹à¦²à§‡ à¦‡à¦‚à¦°à§‡à¦œà¦¿à¦¤à§‡ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¨
6. à¦¯à¦¦à¦¿ à¦‰à¦¤à§à¦¤à¦° à¦œà¦¾à¦¨à¦¾ à¦¨à¦¾ à¦¥à¦¾à¦•à§‡, à¦¤à¦¾à¦¹à¦²à§‡ "à¦†à¦®à¦¿ à¦œà¦¾à¦¨à¦¿ à¦¨à¦¾" à¦¬à¦²à§à¦¨
"""
```

**ğŸ›¡ï¸ Handling Vague Queries:**
1. **ğŸ’­ Conversation Context**: Uses previous exchanges for clarification
2. **â¬‡ï¸ Graceful Degradation**: Returns "à¦†à¦®à¦¿ à¦œà¦¾à¦¨à¦¿ à¦¨à¦¾" for unclear queries
3. **ğŸ“š Source Attribution**: Shows retrieved sources for transparency
4. **ğŸ“Š Confidence Scoring**: Lower confidence for ambiguous queries

</details>

### ğŸ“ˆ Result Relevance and Improvement Strategies

<details>
<summary><strong>ğŸ¯ Do the results seem relevant? What might improve them?</strong></summary>

**ğŸ“Š Current Performance:**
- âœ… Average Groundedness: **0.847**
- âœ… Average Relevance: **0.792**
- âš¡ Response Time: **~1.34 seconds**

**ğŸš€ Improvement Strategies Implemented:**

1. **âœ‚ï¸ Better Chunking:**
   - ğŸ‡§ğŸ‡© Bengali-aware text splitting
   - ğŸ“ Larger chunks with more overlap
   - ğŸ—ï¸ Structure-preserving segmentation

2. **ğŸ¤– Enhanced Embedding:**
   - ğŸ†• Latest OpenAI embedding model
   - ğŸŒ Multilingual optimization
   - ğŸ¯ Domain-specific fine-tuning potential

3. **ğŸ“š Larger Document Base:**
   - ğŸ“„ Multiple document format support
   - ğŸ“– Comprehensive educational content
   - ğŸ”„ Regular content updates

4. **ğŸ” Advanced Retrieval:**
   - ğŸ”€ Hybrid search (semantic + keyword)
   - ğŸ“ˆ Re-ranking mechanisms
   - ğŸ¯ Context-aware filtering

**ğŸ”® Future Improvements:**
- ğŸ¯ Fine-tuned embeddings for educational content
- ğŸ“ˆ Advanced re-ranking models
- ğŸ–¼ï¸ Multi-modal support (images, tables)
- ğŸ§  Real-time learning from user feedback

</details>

---

## ğŸ³ Docker Deployment

### ğŸ—ï¸ Docker Compose Configuration

<details>
<summary>ğŸ“‹ View complete docker-compose.yml</summary>

```yaml
version: '3.8'

services:
  app:
    build: .
    container_name: tedg_app
    expose:
      - '8000'
    env_file:
      - .env
    volumes:
      - ./etc/secrets/build-ai-464207-d3c5fc844bb2.json:/app/etc/secrets/build-ai-464207-d3c5fc844bb2.json:ro
    networks:
      - tedg-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nginx:
    image: nginx:alpine
    container_name: nginx_proxy
    ports:
      - '5008:80'
    networks:
      - tedg-network
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    restart: unless-stopped

networks:
  tedg-network:
    driver: bridge
```

</details>

### ğŸš€ Deployment Commands

```bash
# ğŸ—ï¸ Build and start services
docker-compose up --build

# ğŸŒ™ Run in background
docker-compose up -d

# ğŸ“‹ View logs
docker-compose logs -f

# ğŸ›‘ Stop services
docker-compose down

# ğŸ§¹ Clean up (remove volumes)
docker-compose down -v
```

### ğŸ“Š Monitoring

```bash
# ğŸ“ˆ Check service status
docker-compose ps

# ğŸ’¾ Monitor resource usage
docker stats

# ğŸ” Debug specific service
docker-compose logs app
docker-compose logs nginx
```

---

## ğŸ“ Project Structure

```
ğŸ“¦ sdlfkaskjd/
â”œâ”€â”€ ğŸ¤– chatbot_service/           # Core chatbot functionality
â”‚   â”œâ”€â”€ ğŸ§  chatbot.py            # Main chatbot logic
â”‚   â”œâ”€â”€ ğŸ›£ï¸ chatbot_router.py     # FastAPI routes
â”‚   â””â”€â”€ ğŸ“‹ chatbot_schema.py     # Pydantic models
â”œâ”€â”€ âš™ï¸ config/                   # Configuration management
â”‚   â””â”€â”€ ğŸ”§ config.py
â”œâ”€â”€ ğŸ“š data/                     # Document storage
â”‚   â””â”€â”€ ğŸ“– HSC26-Bangla1st-Paper.txt
â”œâ”€â”€ ğŸ“„ document_processing/      # Document processing routes
â”‚   â””â”€â”€ ğŸ”„ document_extract_router.py
â”œâ”€â”€ ğŸ”¬ others/                   # Evaluation and utilities
â”‚   â””â”€â”€ ğŸ“Š rag_evaluator.py     # RAG evaluation system
â”œâ”€â”€ ğŸ—„ï¸ vectorstore/             # Vector database storage
â”‚   â””â”€â”€ ğŸ“¦ db_faiss_openai_improved/
â”œâ”€â”€ ğŸ”§ create_memory_embedding.py # Vector store creation
â”œâ”€â”€ ğŸš€ main.py                  # FastAPI application entry
â”œâ”€â”€ ğŸ¤– teachbot.py             # Legacy implementation
â”œâ”€â”€ ğŸ“‹ requirements.txt        # Python dependencies
â”œâ”€â”€ ğŸ³ Dockerfile             # Container configuration
â”œâ”€â”€ ğŸ™ docker-compose.yml     # Multi-container setup
â”œâ”€â”€ ğŸŒ nginx.conf            # Nginx configuration
â”œâ”€â”€ ğŸ” .env                  # Environment variables
â””â”€â”€ ğŸ“– README.md             # This documentation
```

---

## ğŸ¯ RAG System Analysis - Detailed Technical Report

### ğŸ“„ Text Extraction Method and Library

**ğŸ› ï¸ Method Used:** Google Cloud Document AI via custom GCP utility wrapper

**ğŸ“š Primary Library:** Google Cloud Document AI API with PyMuPDF as fallback

**âœ… Why This Choice:**

- ğŸ” Google Document AI provides superior OCR capabilities for multilingual content, especially Bengali/Bangla text
- ğŸ“„ Handles multiple document formats (PDF, DOCX, TXT, images) through a unified API
- ğŸ§© Advanced layout analysis and text extraction with chunking support
- ğŸ”„ Fallback mechanisms using PyMuPDF for better Bengali text extraction when Document AI fails

**âš ï¸ Formatting Challenges Faced:**

- ğŸ”¤ Bengali/Bangla character encoding issues requiring multiple encoding attempts (UTF-8, UTF-16, Latin-1, CP1252)
- ğŸ—ï¸ Complex document layouts requiring different extraction methods (direct text, chunked text, layout blocks, page paragraphs)
- ğŸ“ Large PDF files requiring division into smaller chunks (25-page limit for Document AI)
- ğŸ”€ Mixed content types requiring format-specific processing pipelines

### âœ‚ï¸ Chunking Strategy

**ğŸ¯ Strategy Chosen:** Recursive Character Text Splitter with Bengali-aware separators

**âš™ï¸ Configuration:**

- ğŸ“ Chunk size: 1,500 characters (larger for better context)
- ğŸ”— Chunk overlap: 200 characters (for continuity)
- ğŸ‡§ğŸ‡© Custom separators: `["\n\n", "\n", "à¥¤", ".", " ", ""]` (Bengali-aware)

**âœ… Why This Works Well:**

- ğŸ“Š The 1,500 character limit provides sufficient context while staying within embedding model limits
- ğŸ”— 200-character overlap ensures important information isn't lost at chunk boundaries
- ğŸ‡§ğŸ‡© Bengali-specific separators (à¥¤) respect natural language boundaries
- ğŸ—ï¸ Recursive splitting maintains document structure hierarchy
- ğŸ§  Larger chunks work better for semantic retrieval as they preserve more contextual information

### ğŸ¤– Embedding Model

**ğŸ¥‡ Model Used:** OpenAI text-embedding-3-large

**ğŸ¥ˆ Fallback:** HuggingFace sentence-transformers/all-MiniLM-L6-v2

**ğŸŒŸ Why This Choice:**

- ğŸ†• text-embedding-3-large is OpenAI's latest and most capable embedding model
- ğŸŒ Superior multilingual support, crucial for Bengali/Bangla content
- ğŸ“Š Higher dimensional embeddings (3072 dimensions) capture more semantic nuance
- ğŸ“š Better performance on academic and educational content
- âœ… Proven effectiveness for cross-lingual semantic similarity

**ğŸ§  How It Captures Meaning:**

- ğŸ”„ Transformer-based architecture understands contextual relationships
- ğŸŒ Multilingual training enables cross-language semantic understanding
- ğŸ“Š Large parameter count captures subtle semantic distinctions
- ğŸ¯ Contextual embeddings consider surrounding text for better meaning representation

### ğŸ” Similarity Comparison and Storage

**âš¡ Comparison Method:** FAISS (Facebook AI Similarity Search) with cosine similarity

**ğŸ—„ï¸ Storage Setup:** Local FAISS vector database with multiple fallback paths

**âœ… Why This Choice:**

- âš¡ FAISS provides extremely fast similarity search even with large document collections
- ğŸ“ Cosine similarity works well for normalized embeddings from transformer models
- ğŸ”’ Local storage ensures data privacy and fast retrieval
- ğŸ”„ Multiple storage paths provide redundancy and fallback options
- ğŸ’¾ Efficient memory usage and scalable to large document collections

**âš™ï¸ Retrieval Configuration:**

- ğŸ¯ k=5 documents retrieved by default
- ğŸ“Š fetch_k=10 for better candidate selection
- ğŸ”§ Configurable retrieval parameters for different use cases

### ğŸ¯ Meaningful Query-Document Comparison

**ğŸ§  Approach:** Multi-layered semantic matching with context awareness

**ğŸ› ï¸ Implementation:**

- ğŸ” Embedding-based semantic similarity for primary matching
- ğŸ“Š TF-IDF overlap scoring for keyword relevance
- ğŸ’­ Conversation history integration for context continuity
- ğŸ‡§ğŸ‡© Bengali-specific text processing and normalization

**ğŸ¤” Handling Vague or Missing Context:**

- ğŸ’¾ Session-based conversation memory maintains context across queries
- ğŸªŸ Sliding window approach keeps recent conversation history
- ğŸ”„ Fallback to broader semantic search when specific context is missing
- â¬‡ï¸ Graceful degradation with "I don't know" responses for out-of-scope queries
- ğŸ¯ Context-dependent question handling through session management

### ğŸ“ˆ Results Relevance and Improvement Strategies

**ğŸ“Š Current Relevance Assessment:**

Based on the evaluation framework implemented, the system shows:

- âœ… Good groundedness scores through context overlap analysis
- ğŸ¯ Effective relevance matching for Bengali educational content
- ğŸ’ª Strong performance on factual questions about literature and academic topics

**ğŸš€ Potential Improvements:**

1. **âœ‚ï¸ Better Chunking:**
   - ğŸ§  Implement semantic chunking based on topic boundaries
   - ğŸªŸ Use sliding window with dynamic overlap based on content type
   - ğŸ“‹ Add metadata-aware chunking for structured documents

2. **ğŸ¤– Enhanced Embedding Model:**
   - ğŸ¯ Fine-tune embeddings on Bengali educational content
   - ğŸ”€ Implement hybrid retrieval combining dense and sparse methods
   - ğŸ“š Add domain-specific embedding layers for educational terminology

3. **ğŸ“š Larger Document Collection:**
   - ğŸ“– Expand beyond single document to comprehensive curriculum coverage
   - ğŸ“š Add multiple textbooks and reference materials
   - â“ Include question-answer pairs for better educational context

4. **ğŸ” Advanced Retrieval Techniques:**
   - ğŸ“ˆ Implement re-ranking models for better result ordering
   - ğŸ”„ Add query expansion for handling synonyms and related terms
   - ğŸ­ Use ensemble methods combining multiple retrieval strategies

5. **ğŸ¯ Context Enhancement:**
   - ğŸ•¸ï¸ Implement graph-based knowledge representation
   - ğŸ”— Add entity linking for better concept understanding
   - â° Include temporal context for historical and literary content

### ğŸ—ï¸ Technical Architecture Strengths

- **ğŸ›¡ï¸ Robust Error Handling:** Multiple fallback mechanisms at each processing stage
- **ğŸ“ˆ Scalable Design:** Modular architecture supporting different document types and processing methods
- **ğŸŒ Multilingual Support:** Comprehensive Bengali/Bangla text processing capabilities
- **ğŸ“Š Evaluation Framework:** Built-in metrics for groundedness and relevance assessment
- **ğŸ’¾ Memory Management:** Session-based conversation tracking with configurable retention

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

<table>
<tr>
<td width="33%">

### ğŸ› **Bug Reports**
- ğŸ” Check existing issues
- ğŸ“ Provide detailed reproduction steps
- ğŸ–¼ï¸ Include screenshots if applicable

</td>
<td width="33%">

### âœ¨ **Feature Requests**
- ğŸ’¡ Describe the feature clearly
- ğŸ¯ Explain the use case
- ğŸ“‹ Provide implementation suggestions

</td>
<td width="33%">

### ğŸ”§ **Code Contributions**
- ğŸ´ Fork the repository
- ğŸŒ¿ Create a feature branch
- âœ… Add tests if applicable
- ğŸ“¤ Submit a pull request

</td>
</tr>
</table>

### ğŸ“‹ Development Setup

```bash
# ğŸ´ Fork and clone
git clone https://github.com/Nazmul0005/TeachBot-for-Students.git
cd sdlfkaskjd

# ğŸŒ¿ Create feature branch
git checkout -b feature/amazing-feature

# ğŸ”§ Setup development environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# âœ… Run tests
python -m pytest

# ğŸ“¤ Submit changes
git add .
git commit -m "Add amazing feature"
git push origin feature/amazing-feature
```

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

<table>
<tr>
<td width="25%" align="center">

### ğŸ“š **Content**
**Rabindranath Tagore**<br>
*Educational content source*

</td>
<td width="25%" align="center">

### ğŸ¤– **AI Models**
**OpenAI**<br>
*GPT-4 & Embeddings*

</td>
<td width="25%" align="center">

### ğŸ› ï¸ **Framework**
**LangChain**<br>
*RAG implementation*

</td>
<td width="25%" align="center">

### ğŸ” **Search**
**Facebook AI**<br>
*FAISS vector search*

</td>
</tr>
</table>

<div align="center">

### ğŸ“ **Educational Context**
Special thanks to **10 Minute School** for inspiring educational technology innovation

---

<img src="https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg" alt="Made with Love">
<img src="https://img.shields.io/badge/For-Educational%20Excellence-blue.svg" alt="For Educational Excellence">

**Built with â¤ï¸ for educational excellence**

</div>
